# RB-LoRA

**RB-LoRA: Rank-Balanced Aggregation for Low-Rank Adaptation with Federated Fine-Tuning**  
To appear in *Findings of EACL 2026*

This repository hosts the **project webpage** for the RB-LoRA paper.

ðŸ‘‰ **Project page:**  
https://seonha01.github.io/rb-lora/

---

## ðŸ”— Code

The implementation and experimental code are maintained in a **separate repository**:

ðŸ‘‰ **Code repository:**  
https://github.com/seonha01/rb-lora

---

## ðŸ“Œ About

RB-LoRA is a rank-balanced aggregation framework designed for federated fine-tuning with LoRA under heterogeneous client ranks.

Please refer to the **project webpage** for:
- Overview and motivation
- Method illustration
- Experimental results
- Citation information

---

## ðŸ“„ Citation

```bibtex
TBD
}
