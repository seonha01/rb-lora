<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="RB-LoRA: Rank-Balanced Aggregation for Low-Rank Adaptation with Federated Fine-Tuning. A principled framework for aggregating heterogeneous-rank LoRA adapters in federated learning.">
  <meta property="og:title" content="RB-LoRA: Rank-Balanced Aggregation for Low-Rank Adaptation with Federated Fine-Tuning"/>
  <meta property="og:description" content="RB-LoRA addresses rank heterogeneity in federated LoRA by decomposing updates into rank-wise components and aligning them using analytically derived weights."/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/overview.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="RB-LoRA: Rank-Balanced Aggregation for Federated LoRA">
  <meta name="twitter:description" content="A principled framework for aggregating heterogeneous-rank LoRA adapters in federated learning settings.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/overview.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Federated Learning, LoRA, Low-Rank Adaptation, Rank Heterogeneity, Parameter-Efficient Fine-Tuning, PEFT">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>RB-LoRA: Rank-Balanced Aggregation for Low-Rank Adaptation with Federated Fine-Tuning</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">RB-LoRA: Rank-Balanced Aggregation for Low-Rank Adaptation with Federated Fine-Tuning</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://openreview.net/profile?id=%7ESihyeon_Ha2" target="_blank">Sihyeon Ha</a>,</span>
              <span class="author-block">
                <a href="https://sites.google.com/view/yongjeongoh/" target="_blank">Yongjeong Oh</a>,</span>
              <span class="author-block">
                <a href="https://wcml.postech.ac.kr/" target="_blank">Yo-Seb Jeon</a><sup>*</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">Department of Electrical Engineering, POSTECH, South Korea.<br><i>Findings of the Associantion for Computational Linguistics: EACL 2026</i>
              <span class="eql-cntrb"><small><br><sup>*</sup>Corresponding author</small></span></span>
            </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="#" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a> -->
                  <!/span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/seonha01/rb-lora/" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="#" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

                <!-- Video Link -->
                <span class="link-block">
                  <a href="#" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-video"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <video poster="" id="tree" autoplay controls muted loop height="100%"> -->
        <!-- Your video here -->
        <!-- <source src="static/videos/banner_video.mp4" -->
        <!-- type="video/mp4"> -->
      <!-- </video> -->
      <img src="static/images/overview.png"
      class="interpolation-image"
      alt="RB-LoRA Overview"
      style="width: 100%; display: block; margin: 0 auto; margin-bottom: 2rem;" />
      <h2 class="subtitle has-text-centered">
        RB-LoRA addresses rank heterogeneity in federated LoRA by decomposing adapter updates into rank-wise components and aligning them using analytically derived weights based on data volume and rank rarity.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->



<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="title is-3">üìä Main Results</h2>
        <div class="content has-text-justified" style="overflow-x: auto;">
          <table style="min-width: 800px; width: 100%; border-collapse: collapse; margin: 1rem auto;">
            <thead>
              <tr>
                <th style="padding: 12px; border: 1px solid #ccc; text-align: left;"></th>
                <th colspan="2" style="padding: 12px; border: 1px solid #ccc; text-align: center; background-color: #f0f0f0;">Uniform HETLoRA</th>
                <th colspan="2" style="padding: 12px; border: 1px solid #ccc; text-align: center; background-color: #f0f0f0;">Weighted HETLoRA</th>
                <th colspan="2" style="padding: 12px; border: 1px solid #ccc; text-align: center; background-color: #f0f0f0;">FLoRA</th>
                <th colspan="2" style="padding: 12px; border: 1px solid #ccc; text-align: center; background-color: #e8f5e9;"><strong>RB-LoRA</strong></th>
              </tr>
              <tr>
                <th style="padding: 12px; border: 1px solid #ccc; text-align: left;"><strong>Dataset</strong></th>
                <th style="padding: 12px; border: 1px solid #ccc; text-align: center;"><strong>1-Shot</strong></th>
                <th style="padding: 12px; border: 1px solid #ccc; text-align: center;"><strong>3-Shot</strong></th>
                <th style="padding: 12px; border: 1px solid #ccc; text-align: center;"><strong>1-Shot</strong></th>
                <th style="padding: 12px; border: 1px solid #ccc; text-align: center;"><strong>3-Shot</strong></th>
                <th style="padding: 12px; border: 1px solid #ccc; text-align: center;"><strong>1-Shot</strong></th>
                <th style="padding: 12px; border: 1px solid #ccc; text-align: center;"><strong>3-Shot</strong></th>
                <th style="padding: 12px; border: 1px solid #ccc; text-align: center; background-color: #e8f5e9;"><strong>1-Shot</strong></th>
                <th style="padding: 12px; border: 1px solid #ccc; text-align: center; background-color: #e8f5e9;"><strong>3-Shot</strong></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="padding: 12px; border: 1px solid #ccc;">Dolly</td>
                <td style="padding: 12px; border: 1px solid #ccc; text-align: center;">0.53</td>
                <td style="padding: 12px; border: 1px solid #ccc; text-align: center;">0.51</td>
                <td style="padding: 12px; border: 1px solid #ccc; text-align: center;">0.54</td>
                <td style="padding: 12px; border: 1px solid #ccc; text-align: center;">0.52</td>
                <td style="padding: 12px; border: 1px solid #ccc; text-align: center;">0.26</td>
                <td style="padding: 12px; border: 1px solid #ccc; text-align: center;">0.26</td>
                <td style="padding: 12px; border: 1px solid #ccc; text-align: center; background-color: #e8f5e9;"><strong>0.57</strong></td>
                <td style="padding: 12px; border: 1px solid #ccc; text-align: center; background-color: #e8f5e9;"><strong>0.57</strong></td>
              </tr>
              <tr>
                <td style="padding: 12px; border: 1px solid #ccc;">Alpaca</td>
                <td style="padding: 12px; border: 1px solid #ccc; text-align: center;">0.52</td>
                <td style="padding: 12px; border: 1px solid #ccc; text-align: center;">0.52</td>
                <td style="padding: 12px; border: 1px solid #ccc; text-align: center;">0.51</td>
                <td style="padding: 12px; border: 1px solid #ccc; text-align: center;">0.52</td>
                <td style="padding: 12px; border: 1px solid #ccc; text-align: center;">0.31</td>
                <td style="padding: 12px; border: 1px solid #ccc; text-align: center;">0.31</td>
                <td style="padding: 12px; border: 1px solid #ccc; text-align: center; background-color: #e8f5e9;"><strong>0.54</strong></td>
                <td style="padding: 12px; border: 1px solid #ccc; text-align: center; background-color: #e8f5e9;"><strong>0.54</strong></td>
              </tr>
            </tbody>
          </table>
          <p style="margin-top: 1rem; text-align: center;">
            <small><strong>Table 1:</strong> MMLU accuracy evaluated on a 1,444-question subset under 1- and 3-shot communication settings on LLaMa3-8b.</small>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- End paper abstract -->


<!-- Observation -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">üîç Key Contributions</h2>
        <div class="content has-text-justified">
          <ul>
            <li><strong>Unified weighted alignment framework</strong>: We propose a principled framework for heterogeneous rank aggregation that subsumes prior heuristic approaches (zero-padding, replication, stacking).</li>
            <li><strong>Analytically derived weights</strong>: Our factorized weighting scheme is grounded in data volume and rank rarity, providing theoretical justification beyond heuristics.</li>
            <li><strong>Broad validation</strong>: We validate RB-LoRA on federated LoRA for both language models (LLaMA) and vision transformers (ViT), demonstrating consistent improvements.</li>
            <li><strong>Efficient aggregation</strong>: RB-LoRA maintains linear communication and computation complexity while achieving superior accuracy compared to existing methods.</li>
          </ul>

        </div>
      </div>
    </div>
  </div>
</section>





















<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">

              <p>
                Federated fine‚Äëtuning of foundation models is impeded by the need to communicate billions of parameters. Low‚Äërank adaptation (LoRA) alleviates this by updating only compact adapter matrices. However, varying client device capabilities lead to different adapter ranks, causing rank heterogeneity that undermines aggregation, and existing reconciliation methods still incur bias or inefficiency. To address this challenge, we propose \emph{RB-LoRA}, a principled rank‚Äëbalanced aggregation framework that decomposes each update into rank‚Äëwise components and aligns them using analytically derived weights. Experiments on both language and vision models demonstrate consistent improvements under one and three rounds of communication in federated learning.
              </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Problem Statement -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Problem: Rank Heterogeneity in Federated LoRA</h2>
        <div class="content has-text-justified">
               <p>
                In federated LoRA, clients fine-tune and transmit only their adapter matrices, preserving privacy and reducing communication costs. However, clients in practice often adopt different adapter ranks according to their computational capacities, leading to <em>rank heterogeneity</em> where client updates reside in distinct low-dimensional subspaces.
              </p>
              <p>
                Existing methods attempt to heuristically mitigate rank heterogeneity through zero-padding, replication, and stacking. However, these heuristics only try to make updates with different ranks look the same without any analytical basis. As a result, they may unintentionally prioritize either low-rank or high-rank clients, leading to degraded performance. Furthermore, existing studies do not account for data heterogeneity, where client contributions should be weighted by local dataset size according to FedAvg principles.
              </p>
              <p>
                <strong>RB-LoRA</strong> addresses these limitations by formulating rank-balanced aggregation as a weighted-alignment optimization, decomposing adapter updates rank-wise and deriving weights analytically from data volume and rank rarity.
              </p>
              
        </div>
      </div>
    </div>
  </div>
</section>




<!-- Framework -->
<section class="section hero is is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">RB-LoRA Framework</h2>
        <div class="content has-text-justified">
          <img src="static/images/overview.png"
               class="interpolation-image"
               alt="RB-LoRA Framework"
               style="max-width: 100%;" />
               <p></p>
              <p>
                <strong>Rank-Wise Decomposition:</strong> RB-LoRA begins with a rank-wise decomposition of each client's LoRA update, expressing it as a sum of rank-one matrices formed by outer products of basis vectors.
              </p>
              <p>
                <strong>Generalized Aggregation Representation:</strong> We align heterogeneous LoRA updates to a common rank via zero-padding and formulate the aggregated update as a weighted alignment optimization. This unified framework subsumes prior methods (zero-padding, replication, stacking, sketching) as specific choices of the weighting matrix.
              </p>
              <p>
                <strong>Analytically Derived Weights:</strong> Our factorized weighting scheme combines two factors: (1) <em>data proportionality</em> (Œ±<sub>k</sub>), ensuring client contributions scale with local dataset size; (2) <em>rank rarity</em> (Œ≤<sub>r</sub>), compensating for the imbalance where higher-rank components contribute more directions. The weights are derived as Œ≥<sub>r</sub><sup>(k)</sup> = Œ±<sub>k</sub> ¬∑ Œ≤<sub>r</sub>, providing a principled approach beyond heuristics.
              </p>
              <p>
                <strong>Projection:</strong> After aggregation, the global adapter is projected to each client's local rank using SVD-based rank reduction, enabling seamless continuation in subsequent rounds.
              </p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Main result -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experimental Results</h2>
        <div class="content has-text-justified">
		<img src="static/images/main_results2.png"
		class="interpolation-image"
		alt="Experimental Results"
		style="max-width: 100%;"/>
    <p>
      We evaluated <strong>RB-LoRA</strong> on federated fine-tuning of language models (LLaMA2/3) and vision transformers. The experiments were conducted on Alpaca and Dolly datasets distributed across 10 simulated clients with non-IID splits. As shown in Table 1 (above), RB-LoRA consistently outperforms all baselines‚ÄîUniform HETLoRA, Weighted HETLoRA, and FLoRA‚Äîin both 1-shot and 3-shot communication settings.
    </p>
    <p>
      RB-LoRA achieves improvements in MMLU accuracy while maintaining comparable perplexity on WikiText-2 and PTB. The method demonstrates stronger generalization on zero-shot reasoning benchmarks, achieving consistently higher accuracy than other HETLoRA-based methods across both common and advanced task groups.
    </p>
    <p>
      On vision transformers, RB-LoRA also shows consistent improvements, demonstrating its modality-agnostic robustness. The method accelerates convergence and improves final top-1 accuracy on the Food-101 dataset while maintaining linear communication and computation scaling.
    </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Closer look -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Efficiency Analysis</h2>
        <div class="content has-text-justified">
          <div style="overflow-x: auto; margin: 1rem 0;">
            <table style="min-width: 500px; width: 100%; border-collapse: collapse;">
              <thead style="background-color: #f0f0f0;">
                <tr>
                  <th style="padding: 12px; border: 1px solid #ccc;"><strong>Method</strong></th>
                  <th style="padding: 12px; border: 1px solid #ccc;"><strong>#Params/round</strong></th>
                  <th style="padding: 12px; border: 1px solid #ccc;"><strong>Complexity</strong></th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="padding: 12px; border: 1px solid #ccc;">Uniform HETLoRA</td>
                  <td style="padding: 12px; border: 1px solid #ccc;">1.00√ó</td>
                  <td style="padding: 12px; border: 1px solid #ccc;">O(d¬≤KR)</td>
                </tr>
                <tr>
                  <td style="padding: 12px; border: 1px solid #ccc;">Weighted HETLoRA</td>
                  <td style="padding: 12px; border: 1px solid #ccc;">1.00√ó</td>
                  <td style="padding: 12px; border: 1px solid #ccc;">O(d¬≤KR)</td>
                </tr>
                <tr>
                  <td style="padding: 12px; border: 1px solid #ccc;">FLoRA</td>
                  <td style="padding: 12px; border: 1px solid #ccc;">1.57√ó</td>
                  <td style="padding: 12px; border: 1px solid #ccc;">O(dK¬≤R¬≤)</td>
                </tr>
                <tr style="background-color: #e8f5e9;">
                  <td style="padding: 12px; border: 1px solid #ccc;"><strong>RB-LoRA</strong></td>
                  <td style="padding: 12px; border: 1px solid #ccc;"><strong>1.00√ó</strong></td>
                  <td style="padding: 12px; border: 1px solid #ccc;"><strong>O(d¬≤KR)</strong></td>
                </tr>
              </tbody>
            </table>
          </div>
	<p>
    RB-LoRA maintains the same parameter transmission overhead as Uniform/Weighted HETLoRA (1.00√ó) while achieving superior accuracy. In contrast, FLoRA uses about 1.57√ó more global parameters and incurs quadratic aggregation complexity as the number of clients increases. RB-LoRA achieves a better trade-off between accuracy and efficiency, with linear scaling in both communication and computation.
	</p>
	<p> 
		Our factorized weighting scheme uses only O(K + R) parameters, compared to O((KR)¬≤) entries in a full aggregation matrix. Consequently, the aggregation complexity is reduced from O(d(KR)¬≤) to O(d¬≤KR), enabling efficient federated fine-tuning even with a large number of clients.
	</p>
        </div>
      </div>
    </div>
  </div>
</section>
	
<!-- Acknowledgements
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Acknowledgements</h2>
        <div class="content has-text-justified">
          <p>
            This work was partly supported by the National Research Foundation of Korea (NRF) grant funded by the Korean government (MSIT) (RS2023-00213710, RS2023-00210466), and the Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korean government (MSIT) (RS-2019-II191906, Artificial Intelligence Graduate School Program (POSTECH), RS-2022-II220959, Few-Shot learning of Causal Inference in Vision and Language
for Decision Making), and POSCO Creative Ideas grant (2023Q024, 2023Q032).
          </p>
        </div>
      </div>
    </div>
  </div>
</section> -->
  
<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{ha2026rb,
          TBD
        }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the¬†<a href="https://nerfies.github.io" target="_blank">Nerfies</a>¬†project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
