# RB-LoRA Configuration
# Main method: Rank-Based LoRA Aggregation with SVD-based rank reduction

method: "RB-LoRA"
reduction_method: "svd"  # Must use SVD for RB-LoRA

# Model and data
global_model: "meta-llama/Llama-3.1-8B"
data_path: "./data"
dev_data_path: "./mmlu_test_1444.jsonl"
output_dir: "./outputs/rb_lora"

# Federated learning
num_clients: 10
num_communication_rounds: 50
client_selection_strategy: "random"
client_selection_frac: 1.0

# Local training
local_batch_size: 32
local_micro_batch_size: 16
local_num_epochs: 1
local_learning_rate: 2e-5
local_val_set_size: 0
cutoff_len: 512

# LoRA configuration
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules: ["q_proj", "v_proj"]

# LLM configuration
train_on_inputs: true
group_by_length: true
prompt_template_name: "alpaca"
hf_token_json: "./HF_key.json"

# Client configurations (heterogeneous ranks and dataset sizes)
# Example: 10 clients with ranks [4, 4, 16, 16, 64, 64, 128, 128, 256, 256]
client_lora_rank_dict:
  client_0: 4
  client_1: 4
  client_2: 16
  client_3: 16
  client_4: 64
  client_5: 64
  client_6: 128
  client_7: 128
  client_8: 256
  client_9: 256

client_dataset_size_dict:
  client_0: 500
  client_1: 500
  client_2: 1000
  client_3: 1000
  client_4: 1500
  client_5: 1500
  client_6: 1900
  client_7: 1900
  client_8: 2500
  client_9: 2500

